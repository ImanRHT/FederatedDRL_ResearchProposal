%\documentclass{article}
%\usepackage[letterpaper,margin=2.1cm]{geometry}
%\usepackage{xcolor}
%\usepackage{fancyhdr}
%\usepackage{tgschola} % or any other font package you like

\documentclass[12pt]{article}
\usepackage{extsizes}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsxtra}
\usepackage{wasysym}
\usepackage{isomath}
\usepackage{mathtools}
\usepackage{txfonts}
\usepackage{upgreek}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{tensor}
\usepackage{pifont}
\usepackage[margin=15mm]{geometry}
\definecolor{color-1}{rgb}{0.26,0.26,0.26}
\definecolor{color-2}{rgb}{0.4,0.4,0.4}
\usepackage{extsizes}
\usepackage{tocbibind}
\usepackage{float}
\usepackage{flafter}
\usepackage{xcolor}
\usepackage{sectsty}
\usepackage[font=small, skip=0pt]{caption}
\usepackage{setspace}
\setstretch{1.1}
\usepackage{fancyhdr}

\usepackage{nopageno}

% Select the font
\usepackage{charter}


\usepackage[%
square,        % for square brackets
comma,         % use commas as separators
numbers,       % for numerical citations;
%sort           % orders multiple citations into the sequence in which they appear in the list of references;
sort&compress % as sort but in addition multiple numerical citations
% are compressed if possible (as 3-6, 15);
]{natbib}

\renewcommand{\bibfont}{\normalfont\footnotesize}
\usepackage{hyperref}
\hypersetup{
	colorlinks = true,
	citecolor = {blue}
}







\newcommand{\soptitle}{Reinforcement Learning Improves Edge Computing}
\newcommand{\yourname}{Iman Rahmati}
\newcommand{\youremail}{iman.rahmati@sharif.edu}
\newcommand{\yourweb}{\href{https://imanrht.github.io}{imanrht.github.io}}

\newcommand{\statement}[1]{\par\medskip
	\underline{\textcolor{blue}{\textbf{#1:}}}\space
}

%\usepackage[
%colorlinks,
%breaklinks,
%pdftitle={\yourname - \soptitle},
%pdfauthor={\yourname},
%urlcolor  = blue,
%citecolor = blue,
%anchorcolor = blue,
%unicode
%]{hyperref}


\usepackage{setspace}
\onehalfspacing

\begin{document}
	

	
%\pagestyle{fancy}
%\fancyhf{}
%\fancyhead[C]{%
%	\footnotesize\sffamily\vspace{8mm}
%	\textcolor{blue}{\href{mailto:iman.rahmati@sharif.edu}{Research Ideas, V0.1}}  \hfill
%	\textcolor{blue}{\href{https://imanrht.github.io/assets/images/CV_ImanRahmati.pdf}{20 Sep. 2024\vspace{2mm}}}}
%

\begin{center} 
	
	
	\vspace{-17mm}
	
	\large Iman Rahmati  \hfill Federated DRL \vspace{1mm} \hrule
	
	\vspace{-1mm}
	
	
	
	
	\textcolor{white}{i} \\ \LARGE Federated Deep Reinforcement Learning for Continuous Improving Intradependente Task Offloading in Mobile Edge Computing Network\vspace{6mm}\\
	
\end{center}

	
%	\hrule
%	\vspace{0.5pt}
%	\hrule height 1pt



\vspace{-8mm}

\begin{abstract}
	\vspace{-2mm}
	\noindent
	Mobile edge computing often suffers from the dynamic and unknown nature of the environment such as time-varying conditions, heterogeneous devices, and frequent communication requests, imposing significant challenges on improving system performance. To meet the rapidly growing demands of computation-intensive and time-sensitive applications, Reinforcement learning \cite{mnih2015human} has been proposed as an effective tool to establish low-latency and energy-efficient networks. RL enables network entities to interact with the environment and learn an optimal decision-making policy, usually modeled as a Markov decision process \cite{puterman2014markov}.
\end{abstract}



	
\vspace{4mm}

\noindent\large\textbf{Introduction}

\vspace{1.5mm}
\normalsize

Mobile Edge Computing is emerging as a promising paradigm to enhance the computational capacity of mobile devices by offloading tasks to nearby edge servers. This paradigm aims to reduce latency, energy consumption, and improve Quality of Experience (QoE) for end-users. However, one of the major challenges in MEC is the efficient decision-making process for computation offloading, considering the dynamic nature of the network, user demands, and limited resources. Traditional offloading strategies, which often rely on heuristic or single-agent models, fail to capture the com- plexity and stochastic nature of modern MEC systems. 

\newpage

	\noindent\textbf{\large Motivation:  } 
	Federated Reinforcement Learning extends traditional DRL by allowing multiple agents to collaboratively learn a global policy without sharing their local data directly. Each agent makes decisions based on its local observations while cooperating with others to achieve shared system goals. In a Mobile Edge Computing (MEC) environment, edge devices can independently train DRL models using their local data and periodically send updates to a central server. The server aggregates these updates to build a global model, optimizing task offloading across the network.
	
	%\noindent
	%Federated DRL is an extension of DRL that allows multiple agents (or nodes) to collaboratively learn a global policy without sharing their local data. This is achieved by training local models on edge devices and sharing only model updates (e.g., gradients) with a central server to create a global model. The main aim is to enable decentralized training of a DRL model while preserving data privacy and reducing the need for central data storage. In MEC, each edge device could independently train a DRL model based on its local data, then periodically share updates with a central server that aggregates these updates to create a global model for optimized task offloading. 
	%
	%Combining deep reinforcement learning with federated learning yields federated deep reinforcement learning [10] as a new computing paradigm, where each user trains a local DQN and the center trains a general DQN. In each learning iteration, the center sends a policy to each user. By following this policy, each user performs an action and receives a reward. Each user then updates her local DQN using the received reward. Also, each user sends her reward signal back to the center which updates the general 
	%DQN based on these reward signals.
	%
	%Moreover, multi-agent reinforcement learning (MARL) [18] and federated learning (FL) based reinforcement learning [19] are also developed for the distributed scenes where agents learn to make decisions through their local observations and cooperate for the same system targets. Motivated by these, we consider a multi-agent reinforcement learning approach for joint MEC collaboration to maintain the data freshness.
	
	
	\vspace{3mm}
	
	\noindent\textbf{\large Problem Statement: }
	
	
	\vspace{0mm}
	\begin{itemize}
		\item \textbf{Dependency-Aware Task Partitioning}\vspace{-2mm}
		\begin{itemize}
			\item Incorporate a \textbf{Task Call Graph Representation} to account for dependencies among tasks, improving task model accuracy and partitioning effectiveness.
		\end{itemize}
		\vspace{-3mm}
		\item \textbf{Federated Deep Reinforcement Learning}\vspace{-2mm}
		\begin{itemize}
			\item Leverage federated learning for mobile devices in training process. \vspace{-2mm}
			\item Enable mobile devices to collectively contribute to enhancing the offloading model.\vspace{-2mm}
			\item Support continuous learning as new mobile devices join the network.\vspace{-2mm}
		\end{itemize}
	\end{itemize}
	
	\vspace{3mm}
	
	\noindent\textbf{\large Problem Model: } 
	\noindent
	
	
	
	\vspace{5mm}
	
	\noindent\textbf{\large Research Methodology}
	
	\begin{enumerate} \item \textbf{Algorithm Design:} Developing a federated deep reinforcement learning framework for optimizing interdependent task offloading in MEC networks, using teqnics such as DQN or DDPG, which enable network entities to collaboratively learn and adapt offloading strategies without compromising user privacy. \item \textbf{Simulation Environment:}  \item \textbf{Key Challenges:} (a) Communication efficiency, (b) Heterogeneity  
	\end{enumerate}
	



\bibliographystyle{IEEEtranN} % IEEEtranN is the natbib compatible bst file
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{paper}




\end{document}


